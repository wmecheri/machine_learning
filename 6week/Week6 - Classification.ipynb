{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqoHNtBUJ5ez"
   },
   "source": [
    "**Logistic Regression Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VP-mSRwPJ1bl",
    "outputId": "2afdbc60-9ef3-440a-d2c1-69d762083544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities:  [9.99999865e-01 1.00000000e+00 3.60599414e-12 1.36628901e-07]\n",
      "Predicted Class Labels:  [1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (X: features, y: binary labels)\n",
    "X = np.array([[35, 50000], [45, 60000], [25, 30000], [30, 35000]])\n",
    "y = np.array([1, 1, 0, 0])  # 1: Buys product, 0: Does not buy product\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions (probabilities and class labels)\n",
    "y_pred_prob = model.predict_proba(X)[:, 1]  # Probabilities for class 1\n",
    "y_pred = model.predict(X)  # Predicted class labels\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predicted Probabilities: \", y_pred_prob)\n",
    "print(\"Predicted Class Labels: \", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBBz8Jf-J9F6"
   },
   "source": [
    "**Confusion Matrix Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74rtcjiqKAO4",
    "outputId": "1a704c7d-42dd-44ff-af14-6e10657680ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[2 1]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# True labels (y_true) and predicted labels (y_pred)\n",
    "y_true = np.array([1, 0, 1, 1, 0, 0, 1])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1])\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", conf_matrix)\n",
    "\n",
    "# Matrix Breakdown\n",
    "# [ [True Negative, False Positive]\n",
    "#   [False Negative, True Positive] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV1VV10RKGrV"
   },
   "source": [
    "**Performance Metrics (Accuracy, Recall, Precision, Specificity)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7PgI72nKH_k",
    "outputId": "c78487d3-7b9e-4df6-bcc6-456f2affba40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "Specificity: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision (for positive class 1)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Calculate recall (True Positive Rate / Sensitivity)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Calculate specificity (True Negative Rate)\n",
    "# Specificity can be derived manually from confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# f1_score can handle unbalanced data set and should be used instead of accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ69AWTzKSvY"
   },
   "source": [
    "**Threshold and Scores Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XG8L9cdzKWCs",
    "outputId": "2c308c65-67db-47bf-a0bf-4740f61fc6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with Threshold 0.7: [1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Adjust classification threshold for logistic regression\n",
    "\n",
    "threshold = 0.7  # Set threshold\n",
    "y_pred_threshold = (y_pred_prob >= threshold).astype(int)  # Assign class based on threshold\n",
    "\n",
    "print(f\"Predictions with Threshold {threshold}: {y_pred_threshold}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
