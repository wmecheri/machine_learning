{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d27ba5-491f-49e9-a6a0-d999a193f078",
   "metadata": {},
   "source": [
    "# Wassim - Lab 4\n",
    "# Neural Network (Regression)\n",
    "\n",
    "Objective:\n",
    "In this lab, you will use a neural network and a gradient boosting model to predict student performance based on the dataset available at Kaggle: Student Performance - Multiple Linear Regression. The goal is to practice applying and comparing the performance of neural network and gradient boosting regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15be63-9bca-40dd-a879-202fda85f65d",
   "metadata": {},
   "source": [
    "# 0 Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675dbbe6-8274-413e-9d01-32d676ff08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b35bf-9d95-465b-b284-47a442eb03ab",
   "metadata": {},
   "source": [
    "# 1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c36eed-36bf-4618-b39c-cd295d85b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load Dataset\n",
    "df = pd.read_csv('Student_Performance.csv')\n",
    "\n",
    "# 1.2 Handling Missing Values\n",
    "clean_df = df.dropna()\n",
    "\n",
    "# 1.3 Encoding Categorical Variables\n",
    "clean_df.loc[clean_df['Extracurricular Activities']=='No','Extracurricular Activities']=0\n",
    "clean_df.loc[clean_df['Extracurricular Activities']=='Yes','Extracurricular Activities']=1\n",
    "\n",
    "# 1.4 Data Splitting\n",
    "y = np.array(clean_df['Performance Index'], dtype=int)\n",
    "X = np.array(clean_df.drop(columns='Performance Index'), dtype=int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee667c-50e6-4048-b787-0aee174f978d",
   "metadata": {},
   "source": [
    "# 2 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739d280d-dce9-460d-9f14-369c06d1359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Neural Network Model\n",
    "# hidden_layer_sizes define the number of layers and the number of neurones in each of them\n",
    "# max_iter is the maximum number of iterations during training\n",
    "# random_state is the seed to get the same outcome everytime\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000, random_state=0)\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_neural = mlp_reg.predict(X_test)\n",
    "\n",
    "# 2.2 Gradient Boosting Model\n",
    "# n_estimators is the number of estimators, here the number of trees\n",
    "# learning_rate controls the step size that each tree make to correct the error of the model\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gradient = gb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf8cb5-8234-4d6c-afed-d52e5d6ae69e",
   "metadata": {},
   "source": [
    "# 3 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29198b63-c37d-4170-8559-cab7a92bf5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : Neural (4.309024000856876) < Gradient (4.528888593475044)\n",
      "MSA : Neural (1.6490915797344512) < Gradient (1.706846016050602)\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Metrics on Test Data\n",
    "MSE_N = mean_squared_error(y_test, y_pred_neural)\n",
    "MSE_G = mean_squared_error(y_test, y_pred_gradient)\n",
    "MSA_N = mean_absolute_error(y_test, y_pred_neural)\n",
    "MSA_G = mean_absolute_error(y_test, y_pred_gradient)\n",
    "\n",
    "if MSE_N < MSE_G:\n",
    "    print(f\"MSE : Neural ({MSE_N}) < Gradient ({MSE_G})\")\n",
    "else:\n",
    "    print(f\"MSE : Gradient ({MSE_G}) < Neural ({MSE_N})\")\n",
    "\n",
    "if MSA_N < MSA_G:\n",
    "    print(f\"MSA : Neural ({MSA_N}) < Gradient ({MSA_G})\")\n",
    "else:\n",
    "    print(f\"MSA : Gradient ({MSA_G}) < Neural ({MSA_N})\")\n",
    "\n",
    "# 3.2 Comparison and Analysis\n",
    "# Based on the MSE and MSA results, the Neural Network model appears to be better than the Gradient Boosting model in our case.\n",
    "# The results are slightly different and we might need further processing to determine if we are choosing the right model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
