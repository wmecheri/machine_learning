{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44680a89-732d-49e3-93a6-9e7328a5cce9",
   "metadata": {},
   "source": [
    "### Wassim Mecheri Lab 2\n",
    "# Selecting the Best Regression Model for Job Satisfaction Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8400fb4-96cf-4d39-be74-cae7dc44d1b5",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this task is to identify the best model to predict job satisfaction based on employee characteristics. You will compare three models: Lasso, Ridge, and Elastic Net, and find the best penalty value for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b103252-f92f-4f6c-aa1c-f1e097c5f0fd",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e69c7-08fa-4f3d-97bd-17952373addb",
   "metadata": {},
   "source": [
    "We import everything needed fo this Lab:\n",
    "- Pandas for data manipulation and analysis\n",
    "- The function to split the DataSet into train, validation and test\n",
    "- All the needed models (Ridge, Lasso, ElasticNet)\n",
    "- Numpy because we need np.inf\n",
    "- The MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da17a919-d527-4ad6-a725-59c19f6fb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54e374-1488-49d9-afbf-06f6ad77139a",
   "metadata": {},
   "source": [
    "# 1 Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268c41a-dba0-4ede-bc0a-fc3bb5da1ee6",
   "metadata": {},
   "source": [
    "## 1.1 Read the DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b7f8c-a733-4813-881e-c43646aa2208",
   "metadata": {},
   "source": [
    "We first read the DataSet in csv format using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59090e7b-b36a-4c12-a6cb-7b8e2f20562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('modified_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8beed78-90e1-40ca-ae78-956e7af151ac",
   "metadata": {},
   "source": [
    "## 1.2 Create y and X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c77c02-b6e2-4603-a6d2-41e1189bedbc",
   "metadata": {},
   "source": [
    "Then we create the features X and the target y. We must first encode Gender and Education Level since they are not numerical values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc688f0-0bfd-4025-8be2-0bb0393022ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Gender', 'Education_Level'], prefix=['Gender', 'Education_Level'])\n",
    "\n",
    "y = df['Job_Satisfaction']\n",
    "X = df.drop(columns='Job_Satisfaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b915c03-26c8-40f7-b564-ae3717671c93",
   "metadata": {},
   "source": [
    "## 1.3 Split into train, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb08b2c-dcc0-4a4a-8ec5-7395b15df44b",
   "metadata": {},
   "source": [
    "Now now split the data into train, validation and test sets: three for the features and three for the target.\n",
    "We use a temporary set because:\n",
    "- First we cut the data set a first time: 80% for training and 20% for both validation and test\n",
    "- Then we cut the 20% left in half to have 10% for validation and 10% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958a224f-203b-456d-9fd2-2a2ddbc7c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f476c5-76d2-4be8-9987-9eb380a05bad",
   "metadata": {},
   "source": [
    "# 2 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe455d78-0541-4a0f-9a0b-3a1eec9dd5d6",
   "metadata": {},
   "source": [
    "To select the best model:\n",
    "- We generate 10 different penalty values which will be stored in an array\n",
    "- We train the three models using the training data\n",
    "- We try do predictions using the validation data\n",
    "- We calculate and store the best MSE for each to find the best penalty value (Error is everything!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ecc0e-8d66-43f8-8719-bf28268b6a64",
   "metadata": {},
   "source": [
    "## 2.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9ff58e-3e0f-4de9-be26-c12b8141d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmecheri/Documents/Fanshawe/S1/machine_learning/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.836e+01, tolerance: 3.440e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/wmecheri/Documents/Fanshawe/S1/machine_learning/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.366e+01, tolerance: 3.440e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/wmecheri/Documents/Fanshawe/S1/machine_learning/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.697e+02, tolerance: 3.440e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Penalty_Value</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>1291.549665</td>\n",
       "      <td>0.893453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>2.782559</td>\n",
       "      <td>0.928071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>2.782559</td>\n",
       "      <td>0.883021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Models  Penalty_Value       MSE\n",
       "0       Ridge    1291.549665  0.893453\n",
       "1       Lasso       2.782559  0.928071\n",
       "2  ElasticNet       2.782559  0.883021"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty_values = np.logspace(-4, 4, 10) # It generates an array!\n",
    "\n",
    "# To store the best results\n",
    "results_df = pd.DataFrame({'Models': ['Ridge', 'Lasso', 'ElasticNet'], 'Penalty_Value': [np.inf, np.inf, np.inf], 'MSE': [np.inf, np.inf, np.inf]})\n",
    "\n",
    "for penalty_value in penalty_values:\n",
    "    # Create models\n",
    "    ridge = Ridge(alpha = penalty_value)\n",
    "    lasso = Lasso(alpha = penalty_value)\n",
    "    elastic = ElasticNet(alpha = penalty_value, l1_ratio = penalty_value % 1) # 0 <= l1_ratio <= 1\n",
    "\n",
    "    # Train models (with training data!)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    elastic.fit(X_train, y_train)\n",
    "\n",
    "    # Try to predict y (with validation data!)\n",
    "    y_pred_ridge = ridge.predict(X_val)\n",
    "    y_pred_lasso = lasso.predict(X_val)\n",
    "    y_pred_elastic = elastic.predict(X_val)\n",
    "\n",
    "    # Calculate the mse and store if better than previous\n",
    "    mse_ridge = mean_squared_error(y_val, y_pred_ridge)\n",
    "    mse_lasso = mean_squared_error(y_val, y_pred_lasso)\n",
    "    mse_elastic = mean_squared_error(y_val, y_pred_elastic)\n",
    "\n",
    "    if mse_ridge < results_df.loc[results_df['Models'] == 'Ridge', 'MSE'].values[0]:\n",
    "        results_df.loc[results_df['Models'] == 'Ridge', 'Penalty_Value'] = penalty_value\n",
    "        results_df.loc[results_df['Models'] == 'Ridge', 'MSE'] = mse_ridge\n",
    "\n",
    "    if mse_lasso < results_df.loc[results_df['Models'] == 'Lasso', 'MSE'].values[0]:\n",
    "        results_df.loc[results_df['Models'] == 'Lasso', 'Penalty_Value'] = penalty_value\n",
    "        results_df.loc[results_df['Models'] == 'Lasso', 'MSE'] = mse_lasso\n",
    "\n",
    "    if mse_elastic < results_df.loc[results_df['Models'] == 'ElasticNet', 'MSE'].values[0]:\n",
    "        results_df.loc[results_df['Models'] == 'ElasticNet', 'Penalty_Value'] = penalty_value\n",
    "        results_df.loc[results_df['Models'] == 'ElasticNet', 'MSE'] = mse_elastic\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535d071-38c8-4bba-8bf8-d8759e4ccb60",
   "metadata": {},
   "source": [
    "## 2.2 Model Selection Results\n",
    "\n",
    "We return the best penalty value for each model, the MSE looks fairly small given the scale of our target values, so we can assume that the models are good, but we need to try predictions on the test data. We can now try to compare them between each other using the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f9be4-ceef-4a92-9d31-ea732cc7efb8",
   "metadata": {},
   "source": [
    "## 2.3 Personal Notes\n",
    "\n",
    "- 10 values is a very small sample to find the best penalty value, we could use much more values to find a better one\n",
    "- The DataSet is small, only 100 row. With more rows, the results would be better\n",
    "- I have no idea what the warning message is about, and I really dislike warning messages. I hope it’s nothing serious.. :(\n",
    "BUT if we increase the number of iterations as suggested by the error message AND set the l1_ratio parameter to 0.5, apparently a common practice, the warning message disappears! Not sure why, though... :D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd49e6-5f69-4687-89e8-e5e31310065a",
   "metadata": {},
   "source": [
    "# 3 Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af35b9-ad6e-4054-a761-784ee2209f5c",
   "metadata": {},
   "source": [
    "Now that we found the best versions for each model, we can evaluate them on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75687f4a-05c0-44a4-8e77-ac399d284749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge  1.5871022985200427\n",
      "Lasso  4.364248321370532\n",
      "ElasticNet  4.330725570737246\n"
     ]
    }
   ],
   "source": [
    "# Create models using best values\n",
    "penalty_value = results_df.loc[results_df['Models'] == 'Ridge', 'Penalty_Value'].values[0]\n",
    "best_ridge = Ridge(alpha = penalty_value)\n",
    "best_lasso = Lasso(alpha = penalty_value)\n",
    "best_elastic = ElasticNet(alpha = penalty_value, l1_ratio = penalty_value % 1)\n",
    "\n",
    "# Train models (with training data!)\n",
    "best_ridge.fit(X_train, y_train)\n",
    "best_lasso.fit(X_train, y_train)\n",
    "best_elastic.fit(X_train, y_train)\n",
    "\n",
    "# Try to predict y (with test data!)\n",
    "y_pred_best_ridge = best_ridge.predict(X_test)\n",
    "y_pred_best_lasso = best_lasso.predict(X_test)\n",
    "y_pred_best_elastic = best_elastic.predict(X_test)\n",
    "\n",
    "# Calculate the mse and store if better than previous\n",
    "mse_best_ridge = mean_squared_error(y_test, y_pred_best_ridge)\n",
    "mse_best_lasso = mean_squared_error(y_test, y_pred_best_lasso)\n",
    "mse_best_elastic = mean_squared_error(y_test, y_pred_best_elastic)\n",
    "\n",
    "# Print results for test\n",
    "print(\"Ridge \", mse_best_ridge)\n",
    "print(\"Lasso \", mse_best_lasso)\n",
    "print(\"ElasticNet \", mse_best_elastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff95383-041d-4847-a3f6-d3de5512ca33",
   "metadata": {},
   "source": [
    "## 3.1 Model Comparison Results\n",
    "\n",
    "According to the results we got, Ridge sounds better than Lasso and ElasticNet to predict job satisfaction based on employee characteristics: its MSE is much lower than the other two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c974ce-0aeb-4d89-9791-299d085e902e",
   "metadata": {},
   "source": [
    "## 3.2 Personal Notes\n",
    "\n",
    "- Why the MSE are higher now? What happened? \n",
    "- Why do we do train 80%, validation 10% and test 10% when we train on the train data, then do the validation, then the test? It sounds like the same thing to me when we predict on validation and test which are both 10% each, the model never saw validation data like test data, so it's basicaly the same no?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
